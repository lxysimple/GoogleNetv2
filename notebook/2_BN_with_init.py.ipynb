{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)  # reproducible\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers=100, do_bn=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(neural_num) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "        self.do_bn = do_bn\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for (i, linear), bn in zip(enumerate(self.linears), self.bns):\n",
    "            x = linear(x)\n",
    "            if self.do_bn:\n",
    "                x = bn(x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "            print(\"layers:{}, std:{}\".format(i, x.std().item()))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self, mode, std_init=1):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if mode ==\"normal\":\n",
    "                    # method 1\n",
    "                    nn.init.normal_(m.weight.data, std=std_init)    # normal: mean=0, std=1\n",
    "                elif mode == \"kaiming\":\n",
    "                    # method 2 kaiming\n",
    "                    nn.init.kaiming_normal_(m.weight.data)\n",
    "                else:\n",
    "                    print(\"不支持{}输入\".format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:0.575767457485199\n",
      "layers:1, std:0.5824317932128906\n",
      "layers:2, std:0.5823703408241272\n",
      "layers:3, std:0.5815761089324951\n",
      "layers:4, std:0.5813344120979309\n",
      "layers:5, std:0.5830932855606079\n",
      "layers:6, std:0.5783577561378479\n",
      "layers:7, std:0.585452675819397\n",
      "layers:8, std:0.5750972032546997\n",
      "layers:9, std:0.5790500640869141\n",
      "layers:10, std:0.5800771713256836\n",
      "layers:11, std:0.5729996562004089\n",
      "layers:12, std:0.581875741481781\n",
      "layers:13, std:0.5765160918235779\n",
      "layers:14, std:0.5772631168365479\n",
      "layers:15, std:0.5824704170227051\n",
      "layers:16, std:0.575861394405365\n",
      "layers:17, std:0.5708439946174622\n",
      "layers:18, std:0.5818034410476685\n",
      "layers:19, std:0.5798003077507019\n",
      "layers:20, std:0.5792176127433777\n",
      "layers:21, std:0.5766910314559937\n",
      "layers:22, std:0.5747811198234558\n",
      "layers:23, std:0.588204562664032\n",
      "layers:24, std:0.581583559513092\n",
      "layers:25, std:0.5788277387619019\n",
      "layers:26, std:0.5830960869789124\n",
      "layers:27, std:0.5800673961639404\n",
      "layers:28, std:0.5785953402519226\n",
      "layers:29, std:0.5843232274055481\n",
      "layers:30, std:0.5780155062675476\n",
      "layers:31, std:0.577354371547699\n",
      "layers:32, std:0.5781227946281433\n",
      "layers:33, std:0.5826283693313599\n",
      "layers:34, std:0.584625780582428\n",
      "layers:35, std:0.580120861530304\n",
      "layers:36, std:0.5778435468673706\n",
      "layers:37, std:0.5865468978881836\n",
      "layers:38, std:0.5906266570091248\n",
      "layers:39, std:0.5808035135269165\n",
      "layers:40, std:0.585400402545929\n",
      "layers:41, std:0.5773333311080933\n",
      "layers:42, std:0.5857028961181641\n",
      "layers:43, std:0.5713160037994385\n",
      "layers:44, std:0.5810115337371826\n",
      "layers:45, std:0.5812532305717468\n",
      "layers:46, std:0.5809859037399292\n",
      "layers:47, std:0.5810487866401672\n",
      "layers:48, std:0.5829777121543884\n",
      "layers:49, std:0.5810613632202148\n",
      "layers:50, std:0.5843528509140015\n",
      "layers:51, std:0.5818418264389038\n",
      "layers:52, std:0.5800071954727173\n",
      "layers:53, std:0.5778676271438599\n",
      "layers:54, std:0.5755739212036133\n",
      "layers:55, std:0.5843010544776917\n",
      "layers:56, std:0.5741264224052429\n",
      "layers:57, std:0.5816507339477539\n",
      "layers:58, std:0.5783504843711853\n",
      "layers:59, std:0.5919751524925232\n",
      "layers:60, std:0.5843228697776794\n",
      "layers:61, std:0.5846503376960754\n",
      "layers:62, std:0.5757527351379395\n",
      "layers:63, std:0.57444828748703\n",
      "layers:64, std:0.5774610042572021\n",
      "layers:65, std:0.5765696167945862\n",
      "layers:66, std:0.5862876772880554\n",
      "layers:67, std:0.5844489932060242\n",
      "layers:68, std:0.5837051272392273\n",
      "layers:69, std:0.5799015760421753\n",
      "layers:70, std:0.5798283219337463\n",
      "layers:71, std:0.576780378818512\n",
      "layers:72, std:0.5757566094398499\n",
      "layers:73, std:0.5769439339637756\n",
      "layers:74, std:0.5756057500839233\n",
      "layers:75, std:0.5795183777809143\n",
      "layers:76, std:0.5784865617752075\n",
      "layers:77, std:0.5819503664970398\n",
      "layers:78, std:0.5754809379577637\n",
      "layers:79, std:0.5853989720344543\n",
      "layers:80, std:0.5807756781578064\n",
      "layers:81, std:0.5838733315467834\n",
      "layers:82, std:0.5768408179283142\n",
      "layers:83, std:0.5813587307929993\n",
      "layers:84, std:0.5678582191467285\n",
      "layers:85, std:0.5873841643333435\n",
      "layers:86, std:0.5803330540657043\n",
      "layers:87, std:0.582958459854126\n",
      "layers:88, std:0.5768753290176392\n",
      "layers:89, std:0.5745313167572021\n",
      "layers:90, std:0.5787172317504883\n",
      "layers:91, std:0.5847108960151672\n",
      "layers:92, std:0.5827808380126953\n",
      "layers:93, std:0.5789850950241089\n",
      "layers:94, std:0.581640362739563\n",
      "layers:95, std:0.5812875032424927\n",
      "layers:96, std:0.5812014937400818\n",
      "layers:97, std:0.5784167647361755\n",
      "layers:98, std:0.5730496048927307\n",
      "layers:99, std:0.5776208639144897\n",
      "tensor([[0.2231, 0.5969, 1.5886,  ..., 0.2919, 0.0000, 0.0923],\n",
      "        [0.8662, 1.3250, 0.0000,  ..., 0.5773, 1.0444, 0.0000],\n",
      "        [0.7126, 0.9418, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5763, 0.1906,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1983,  ..., 1.9421, 0.0000, 0.0000],\n",
      "        [1.3105, 1.6786, 0.0000,  ..., 1.1911, 0.2662, 0.5968]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    neural_nums = 256\n",
    "    layer_nums = 100\n",
    "    batch_size = 16\n",
    "\n",
    "    net = MLP(neural_nums, layer_nums, do_bn=False)      # 1. 无初始化； # 2. normal_初始化； # 3。 kaiming初始化\n",
    "#     net = MLP(neural_nums, layer_nums, do_bn=True)        # 4. BN+无初始化； 5. BN + normal; 6. BN + kaiming, 7. BN+1000\n",
    "#     net.initialize(\"normal\", std_init=1)\n",
    "#     net.initialize(\"normal\", std_init=10000)\n",
    "#     net.initialize(\"kaiming\")\n",
    "\n",
    "    inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "    output = net(inputs)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察神经网络神经元数据尺度变化\n",
    "\n",
    "<font  size=12 face=\"黑体\">\n",
    "    \n",
    "有无BN层 | 无初始化 | N(0, 1) | Kaiming初始化 | N(0, 10000)\n",
    ":-: | :-: | :-: | :-: | :-:\n",
    "无BN层| 1e-40 | NaN in 35 layers | 0.4 | NaN in 8 layers| \n",
    "有BN层 | 0.57 | 0.57 | 0.57 |0.57|\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.4_gpu",
   "language": "python",
   "name": "pytorch_1.4_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
