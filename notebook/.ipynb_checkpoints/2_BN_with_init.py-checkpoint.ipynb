{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)  # reproducible\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers=100, do_bn=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(neural_num) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "        self.do_bn = do_bn\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for (i, linear), bn in zip(enumerate(self.linears), self.bns):\n",
    "            x = linear(x)\n",
    "            if self.do_bn:\n",
    "                x = bn(x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "            print(\"layers:{}, std:{}\".format(i, x.std().item()))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self, mode, std_init=1):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if mode ==\"normal\":\n",
    "                    # method 1\n",
    "                    nn.init.normal_(m.weight.data, std=std_init)    # normal: mean=0, std=1\n",
    "                elif mode == \"kaiming\":\n",
    "                    # method 2 kaiming\n",
    "                    nn.init.kaiming_normal_(m.weight.data)\n",
    "                else:\n",
    "                    print(\"不支持{}输入\".format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:0.5848153233528137\n",
      "layers:1, std:0.5827966928482056\n",
      "layers:2, std:0.5787407755851746\n",
      "layers:3, std:0.583806574344635\n",
      "layers:4, std:0.5799300074577332\n",
      "layers:5, std:0.5819533467292786\n",
      "layers:6, std:0.5800478458404541\n",
      "layers:7, std:0.5818994045257568\n",
      "layers:8, std:0.5750249028205872\n",
      "layers:9, std:0.5772916674613953\n",
      "layers:10, std:0.5853444337844849\n",
      "layers:11, std:0.5787179470062256\n",
      "layers:12, std:0.5728684663772583\n",
      "layers:13, std:0.5785820484161377\n",
      "layers:14, std:0.5855928063392639\n",
      "layers:15, std:0.5862574577331543\n",
      "layers:16, std:0.5794041156768799\n",
      "layers:17, std:0.5808504223823547\n",
      "layers:18, std:0.5883684754371643\n",
      "layers:19, std:0.5889952778816223\n",
      "layers:20, std:0.5744621753692627\n",
      "layers:21, std:0.5760332942008972\n",
      "layers:22, std:0.5747781991958618\n",
      "layers:23, std:0.5878269672393799\n",
      "layers:24, std:0.5815837383270264\n",
      "layers:25, std:0.5916622281074524\n",
      "layers:26, std:0.5802280306816101\n",
      "layers:27, std:0.5706748962402344\n",
      "layers:28, std:0.579828143119812\n",
      "layers:29, std:0.580768883228302\n",
      "layers:30, std:0.5813040733337402\n",
      "layers:31, std:0.5807573199272156\n",
      "layers:32, std:0.5808411240577698\n",
      "layers:33, std:0.5811774134635925\n",
      "layers:34, std:0.5778232216835022\n",
      "layers:35, std:0.5735552906990051\n",
      "layers:36, std:0.5775420665740967\n",
      "layers:37, std:0.580765426158905\n",
      "layers:38, std:0.5762114524841309\n",
      "layers:39, std:0.5848813652992249\n",
      "layers:40, std:0.5816558599472046\n",
      "layers:41, std:0.5774251222610474\n",
      "layers:42, std:0.5789108872413635\n",
      "layers:43, std:0.5824865698814392\n",
      "layers:44, std:0.5797836184501648\n",
      "layers:45, std:0.5688650608062744\n",
      "layers:46, std:0.5756446719169617\n",
      "layers:47, std:0.578741192817688\n",
      "layers:48, std:0.5723577737808228\n",
      "layers:49, std:0.5809733867645264\n",
      "layers:50, std:0.5758345723152161\n",
      "layers:51, std:0.5756662487983704\n",
      "layers:52, std:0.5796993970870972\n",
      "layers:53, std:0.5766191482543945\n",
      "layers:54, std:0.5777311325073242\n",
      "layers:55, std:0.5769439935684204\n",
      "layers:56, std:0.5820730924606323\n",
      "layers:57, std:0.5655356049537659\n",
      "layers:58, std:0.586699903011322\n",
      "layers:59, std:0.5766400098800659\n",
      "layers:60, std:0.5793126225471497\n",
      "layers:61, std:0.576240062713623\n",
      "layers:62, std:0.5815858840942383\n",
      "layers:63, std:0.5764245390892029\n",
      "layers:64, std:0.5803271532058716\n",
      "layers:65, std:0.5845659971237183\n",
      "layers:66, std:0.5776547193527222\n",
      "layers:67, std:0.5833892822265625\n",
      "layers:68, std:0.5856022238731384\n",
      "layers:69, std:0.5817444324493408\n",
      "layers:70, std:0.5710830688476562\n",
      "layers:71, std:0.5778003334999084\n",
      "layers:72, std:0.5765707492828369\n",
      "layers:73, std:0.5871753692626953\n",
      "layers:74, std:0.5827845335006714\n",
      "layers:75, std:0.5754667520523071\n",
      "layers:76, std:0.5860546827316284\n",
      "layers:77, std:0.5813757181167603\n",
      "layers:78, std:0.5835583806037903\n",
      "layers:79, std:0.5792884826660156\n",
      "layers:80, std:0.5746603608131409\n",
      "layers:81, std:0.5742409229278564\n",
      "layers:82, std:0.5902440547943115\n",
      "layers:83, std:0.5837833881378174\n",
      "layers:84, std:0.5809357762336731\n",
      "layers:85, std:0.587340772151947\n",
      "layers:86, std:0.5772221684455872\n",
      "layers:87, std:0.5801150798797607\n",
      "layers:88, std:0.5869463086128235\n",
      "layers:89, std:0.576662540435791\n",
      "layers:90, std:0.5873810052871704\n",
      "layers:91, std:0.5810315608978271\n",
      "layers:92, std:0.580788254737854\n",
      "layers:93, std:0.5797101259231567\n",
      "layers:94, std:0.5774056911468506\n",
      "layers:95, std:0.5821540355682373\n",
      "layers:96, std:0.574370801448822\n",
      "layers:97, std:0.593005359172821\n",
      "layers:98, std:0.5854688882827759\n",
      "layers:99, std:0.574325442314148\n",
      "tensor([[1.9119, 0.6480, 0.4540,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1094, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.7220,  ..., 0.0000, 0.0000, 1.4657],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.3820,  ..., 0.0000, 0.1835, 0.6268],\n",
      "        [0.0000, 0.0000, 0.7651,  ..., 0.0000, 0.7993, 0.1647],\n",
      "        [0.2690, 0.0000, 0.0000,  ..., 0.4956, 0.0000, 0.6875]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    neural_nums = 256\n",
    "    layer_nums = 100\n",
    "    batch_size = 16\n",
    "\n",
    "    net = MLP(neural_nums, layer_nums, do_bn=False)      # 1. 无初始化； # 2. normal_初始化； # 3。 kaiming初始化\n",
    "#     net = MLP(neural_nums, layer_nums, do_bn=True)        # 4. BN+无初始化； 5. BN + normal; 6. BN + kaiming, 7. BN+1000\n",
    "#     net.initialize(\"normal\", std_init=1)\n",
    "#     net.initialize(\"normal\", std_init=1000)\n",
    "#     net.initialize(\"kaiming\")\n",
    "\n",
    "    inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "    output = net(inputs)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察神经网络神经元数据尺度变化\n",
    "\n",
    "<font  size=12 face=\"黑体\">\n",
    "    \n",
    "有无BN层 | 无初始化 | N(0, 1) | Kaiming初始化 | N(0, 10000)\n",
    ":-: | :-: | :-: | :-: | :-:\n",
    "无BN层| 1e-19 | NaN in 35 layers | 0.4 | NaN in 8 layers| \n",
    "有BN层 | 0.57 | 0.57 | 0.57 |0.57|\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_test",
   "language": "python",
   "name": "pytorch_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
